{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Iris* flower classification with micrograd\n",
    "In this notebook, you will implement a 2-layer (4-16-3) fully connected\n",
    "feed-forward neural network to classify species of of *Iris* flowers, based on\n",
    "four different measurements (sepal length, sepal width, petal length, and petal\n",
    "width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import micrograd.nn as nn\n",
    "from micrograd.engine import Value\n",
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's setup our dataset. \n",
    "\n",
    "The \"x\" variables are the images, while the \"y\" variables are the ground truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape=(120, 4)\n",
      "train_y.shape=(120,)\n",
      "test_x.shape=(30, 4)\n",
      "test_y.shape=(30,)\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "train_x, test_x, train_y, test_y = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "print(f\"{train_x.shape=}\\n{train_y.shape=}\\n{test_x.shape=}\\n{test_y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data loaded, we can initialize our model, using the\n",
    "abstractions we wrote in `micrograd/nn.py` (imported above as `nn`).\n",
    "\n",
    "Remember that we are looking to create a multi-layer perceptron, with one hidden\n",
    "layer of dimension 16, an input layer of 4, and output layer of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.MLP(4, [16, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try evaluating our model on the first flower in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=1.3497568231790034, grad=0),\n",
       " Value(data=-10.558013135005577, grad=0),\n",
       " Value(data=1.1832076310091515, grad=0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, of course, this returns total garbage, because our model is just running on\n",
    "initialized random values for its weights. First, let's make sure that our model\n",
    "is actually outputting a list of probabilities for each digit. We will use the\n",
    "softmax function to do this:\n",
    "$$\\sigma(z)_i=\\frac{\\exp(z_i)}{\\sum_{c=1}^C\\exp(z_c)}$$\n",
    "Where $C$ is number of classes (3). Implement this function below. Remember,\n",
    "you are working with `Value`s, so you can only use the operations you defined\n",
    "in `micrograd/engine.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: list[Value], C:int=10) -> list[Value]:\n",
    "  denom = sum(list(zi.exp() for zi in z))\n",
    "  return list(zi.exp() / denom for zi in z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try evaluating the model again, with our new softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.5415393414771973, grad=0),\n",
       " Value(data=3.6488098187818702e-06, grad=0),\n",
       " Value(data=0.458457009712984, grad=0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(model(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a little better, doesn't it? Of course, it's still not an accurate\n",
    "prediction, but at least it is a prediction. Now we need to worry about training\n",
    "the model. \n",
    "\n",
    "In order to train the model, we first need to find a way to measure how wrong\n",
    "its predictions are, that is we need to define our cost (loss) function. We will\n",
    "use cross entropy loss, which is defined by the following equation:\n",
    "$$\\ell (H(x),y)=-\\log\\frac{\\exp(H(x)_{y})}{\\sum_{c=1}^C\\exp(H(x)_c)}$$\n",
    "Where $x$ is the input image, $H$ is our model, $y$ is the ground truth\n",
    "corresponding to $x$, and $C$ is the number of classes (3). But note that this\n",
    "is the same as:\n",
    "$$\\ell (H(x),y)=-\\log(\\sigma(H(x))_y)$$\n",
    "Where $\\sigma$ is softmax from above.\n",
    "\n",
    "Remember that since you are working with `Value`'s you only have access to the\n",
    "operations defined in `micrograd/engine.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred: list[Value], truth: int) -> Value:\n",
    "  return -(softmax(pred)[truth].log())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our loss function is defined, finally, we can write the code for\n",
    "gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Accuracy=0.36666666666666664, Loss=7.824446113869851\n",
      "Epoch 10: Accuracy=0.7, Loss=0.5285752809681077\n",
      "Epoch 20: Accuracy=0.7, Loss=0.43878918785606336\n",
      "Epoch 30: Accuracy=0.8333333333333334, Loss=0.3698044685069986\n",
      "Epoch 40: Accuracy=1.0, Loss=0.3311467328890145\n",
      "Epoch 50: Accuracy=1.0, Loss=0.30756669967310735\n",
      "Epoch 60: Accuracy=1.0, Loss=0.2897295185150001\n",
      "Epoch 70: Accuracy=1.0, Loss=0.27605731900340535\n",
      "Epoch 80: Accuracy=1.0, Loss=0.2659335484175744\n",
      "Epoch 90: Accuracy=1.0, Loss=0.25890412924031925\n"
     ]
    }
   ],
   "source": [
    "def argmax(arr: list[Value]) -> int:\n",
    "  return arr.index(max(arr, key=lambda x: x.data))\n",
    "\n",
    "for epoch in range(100):\n",
    "  # forward\n",
    "  scores = list(map(lambda x: model(x), train_x))\n",
    "  losses = list(map(lambda tup: loss(tup[1], train_y[tup[0]]), enumerate(scores)))\n",
    "  total_loss = sum(losses) * (1.0 / len(losses))\n",
    "\n",
    "  # backward\n",
    "  model.zero_grad()\n",
    "  total_loss.backward()\n",
    "\n",
    "  # update (sgd)\n",
    "  learning_rate = (1.0 - 0.9*epoch/100) / 10\n",
    "  for p in model.parameters():\n",
    "    p.data -= learning_rate * p.grad\n",
    "\n",
    "  if epoch % 10 == 0: print(f\"Epoch {epoch}: Accuracy={sum([argmax(softmax(model(x))) == y for x, y in zip(test_x, test_y)])/len(test_y)}, Loss={total_loss.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Final accuracy:\", sum([argmax(softmax(model(x))) == y for x, y in zip(test_x, test_y)])/len(test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-fellowship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
