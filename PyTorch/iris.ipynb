{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Iris* flower classification with PyTorch\n",
    "In this notebook, you will reimplement your 3-layer (4-16-16-3) fully connected\n",
    "network from the first project. We will skip some of the steps for the sake of\n",
    "brevity, this is just to get you familiar with the PyTorch environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cpu\") # Put your device string here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first setup our dataset as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "train_x, test_x, train_y, test_y = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "train_x = torch.from_numpy(train_x).to(torch.float32)\n",
    "train_y = torch.from_numpy(train_y).to(torch.long)\n",
    "test_x = torch.from_numpy(test_x).to(torch.float32)\n",
    "test_y = torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define our new PyTorch model! In Torch, models are defined as\n",
    "classes that extend `nn.Module`, similar to how we defined our MLP in micrograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IrisNet(nn.Module):\n",
    "  # First, we must define our constructor.\n",
    "  def __init__(self):\n",
    "    super(IrisNet, self).__init__()\n",
    "    # TODO: Define our layers here (4, 16, 16, 3)\n",
    "    self.layer1 = nn.Linear(4, 16)\n",
    "    self.layer2 = nn.Linear(16, 16)\n",
    "    self.layer3 = nn.Linear(16, 3)\n",
    "  \n",
    "  # Now we need to instruct how to compute the forward pass.\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer1(x))\n",
    "    x = F.relu(self.layer2(x))\n",
    "    x = self.layer3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is defined, we can instantiate it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IrisNet().to(DEVICE) # The to() method allows us to do calculations on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's try evaluating our model on the first flower in the training set.\n",
    "We don't need to implement our own softmax function this time, as it is built-in\n",
    "to PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9500\\2117899929.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(model(train_x[0]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4786, 0.3113, 0.2101], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(model(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we need to train the model. But torch makes this easy with built-in\n",
    "optimizers and loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Accuracy: 0.36666666666666664\n",
      "Epoch 100 | Accuracy: 0.675\n",
      "Epoch 200 | Accuracy: 0.8583333333333333\n",
      "Epoch 300 | Accuracy: 0.9333333333333333\n",
      "Epoch 400 | Accuracy: 0.9666666666666667\n",
      "Epoch 500 | Accuracy: 0.975\n",
      "Epoch 600 | Accuracy: 0.9916666666666667\n",
      "Epoch 700 | Accuracy: 0.9916666666666667\n",
      "Epoch 800 | Accuracy: 0.9916666666666667\n",
      "Epoch 900 | Accuracy: 0.9916666666666667\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # In practice, we can use other optimizers, such as Adam.\n",
    "\n",
    "for epoch in range(1000):\n",
    "  # forward\n",
    "  scores = model(train_x) # Note we can just pass the whole dataset at once!\n",
    "  loss = loss_fn(scores, F.one_hot(train_y).float())\n",
    "\n",
    "  # backward\n",
    "  optimizer.zero_grad() # Zero out the gradients.\n",
    "  loss.backward() # Compute the gradients.\n",
    "  optimizer.step() # Update the parameters automatically!\n",
    "\n",
    "  if epoch % 100 == 0:\n",
    "    print(f\"Epoch {epoch} | Accuracy: {torch.sum(torch.argmax(scores, dim=1) == train_y).item() / len(train_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Final accuracy:\", torch.sum(torch.argmax(model(test_x), dim=1) == test_y).item() / len(test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-fellowship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
